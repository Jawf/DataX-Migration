source.db.url=jdbc:mysql://xxx.xxx.xxx.xxx:3306/sourcedbname?useUnicode=true&characterEncoding=UTF-8
source.db.name=sourcedbname
source.db.username=username
source.db.password=password
target.db.url=jdbc:mysql://xxx.xxx.xxx.xxx:3306/targetdbname?useUnicode=true&characterEncoding=UTF-8
target.db.name=targetdbname
target.db.username=username
target.db.password=password
#global where clause to filter the migration data, the clause also be used in get status of report, ensure it able to be run in source and target db
#if the source table contain the column in where clause, will use the first where clause and ignore the second.
#if the source table does not contain the column in the where clause, but contain column in the second clause, will use the second clause and igonre the first clause.
#if the source table does not contain both where column, will ignore both
source.db.global.where.clause=""
source.db.global.where.second.clause=""
#target db query sql: select migration tables
migration.query.target.tables.sql=select ut.table_name from information_schema.tables ut where ut.table_schema='targetdbname' and ut.table_type='base table' 
#target db query sql: select migration table columns
migration.query.target.table.columns.sql=select column_name from information_schema.columns t where table_schema='targetdbname' and table_name='{0}'
#target db query sql: select migration table primary keys
migration.query.target.table.primarykeys.sql=select column_name from information_schema.columns t where column_key='pri' and table_name='{0}'
#must contain 1:tablename,2:size,3:numOfRows. And must order by size desc.
#mysql query
migration.query.source.tables.status.sql=select ut.table_name,(ut.data_length+ut.index_length)/1024/1024 as size_MB, ut.table_rows  from information_schema.tables ut where ut.table_schema='targetdbname' and ut.table_type='base table' order by size_MB desc;
#oracle query
#migration.query.source.tables.status.sql=select * from ( select ut.table_name, nvl(us.size_MB,0) as size_MB, uta.num_rows from user_tab_comments ut left join (select segment_name as TableName, sum(bytes)/(1024*1024) as Size_MB  from user_segments where segment_type like 'TABLE%' group by segment_name) us on ut.table_name=us.TableName  left join user_tables uta on uta.table_name=ut.table_name where ut.table_type = 'TABLE' )order by Size_MB desc
#mutiple channel used within one job to speed the migration, 2 channel will open 2*5 thread for one job. Caution: Mutiple channels may able to cause records consistency. 
migration.datax.channel.multiple=true
migration.datax.channel.2channels.records.over=1000000
migration.datax.channel.4channels.records.over=10000000
migration.datax.channel.nchannels.number=8
migration.datax.channel.nchannels.records.over=100000000
migration.datax.tool.folder=E:/work/db_migration/datax/datax
#if got error when migration, identity whether go on or not 
migration.error.continue=true
#define the tables that ingore to migration
migration.ingore.tables=empty
#define the table size bigger than the value and ignore to migration
migration.ingore.bigtables.size.than.mb=1
#job thread to group a number of tables in thread by split type, available value: index:tables list index in the cvs reports, size:table size
migration.jobthread.split.type=size
#job thread max tables, if between size:20-10 got 60 tables, will be grouped to 40,20. similar for split by index numbers. Adjust this value according to the big table size to encrease the migration speed.
migration.jobthread.split.maxcount=40
#enabled when type=size
migration.jobthread.split.tablesize.mb="40000,30000,20000,10000,5000,1000,500,200,100,50,20,10,1,0.4375,0.25,0.1875,0.125,0.0625"
#enabled when type=index
migration.jobthread.split.indexes="0,1,2,5,10,50,60,90,100,200,300,310"
